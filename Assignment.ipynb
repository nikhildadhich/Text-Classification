{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction from 20 newsgroups documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=[\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"all\",\"am\",\"an\",\"and\",\"any\",\"are\",\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\n",
    "\"by\",\"could\",\"did\",\"do\",\"does\",\"doing\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\"further\",\"had\",\"has\",\"have\",\"having\",\"he\",\"he'd\",\"he'll\",\"he's\",\"her\",\n",
    "\"here\",\"here's\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"how's\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"if\",\"in\",\"into\",\"is\",\"it\",\"it's\",\"its\",\"itself\",\"let's\",\"me\",\n",
    "\"more\",\"most\",\"my\",\"myself\",\"nor\",\"of\",\"on\",\"once\",\"only\",\"or\",\"other\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"same\",\"she\",\n",
    "\"she'd\",\"she'll\",\"she's\",\"should\",\"so\",\"some\",\"such\",\"than\",\"that\",\"that's\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"there's\",\n",
    "\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"this\",\"those\",\"through\",\"to\",\"too\",\"under\",\"until\",\"up\",\"very\",\"was\",\"we\",\"we'd\",\n",
    "\"we'll\",\"we're\",\"we've\",\"were\",\"what\",\"what's\",\"when\",\"when's\",\"where\",\"where's\",\"which\",\"while\",\"who\",\"who's\",\"whom\",\"why\",\"why's\",\"with\",\n",
    "\"would\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = [f for f in os.listdir('C:/Users/nikhi/Downloads/20_newsgroups') if not f.startswith('.')]\n",
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "for i in range(len(directory)):\n",
    "    ##Create a list of files in the given dictionary \n",
    "    files = os.listdir('C:/Users/nikhi/Downloads/20_newsgroups/' + directory[i])\n",
    " \n",
    "    for j in range(len(files)):\n",
    "        ##Path of each file \n",
    "        path = 'C:/Users/nikhi/Downloads/20_newsgroups/' + directory[i] + '/' + files[j]\n",
    "        \n",
    "        ##open the file and read it\n",
    "        text = open(path, 'r', errors='ignore').read()\n",
    "        \n",
    "        for word in text.split():\n",
    "            if len(word) != 1: \n",
    "                ##Check if word is a non stop word or non block word(we have created) only then proceed\n",
    "                if not word.lower() in stop_words:     \n",
    "                    ##If word is already in dictionary then we just increment its frequency by 1\n",
    "                    if vocab.get(word.lower()) != None:\n",
    "                        vocab[word.lower()] += 1\n",
    "\n",
    "                    ##If word is not in dictionary then we put that word in our dictinary by making its frequnecy 1\n",
    "                    else:\n",
    "                        vocab[word.lower()] = 1\n",
    "            \n",
    "# vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted_vocab = sorted(vocab.items(), key= operator.itemgetter(1), reverse= True)\n",
    "# sorted_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary containing the most occuring k-words.\n",
    "kvocab={}\n",
    "\n",
    "# Frequency of 1000th most occured word\n",
    "z = sorted_vocab[2000][1]\n",
    "\n",
    "for x in sorted_vocab:\n",
    "    kvocab[x[0]] = x[1]\n",
    "    \n",
    "    if x[1] <= z:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('not', 29566),\n",
       " ('subject:', 20486),\n",
       " ('from:', 20417),\n",
       " ('date:', 20137),\n",
       " ('newsgroups:', 20081),\n",
       " ('message-id:', 20050),\n",
       " ('lines:', 20042),\n",
       " ('path:', 20029),\n",
       " ('apr', 19602),\n",
       " ('organization:', 19246),\n",
       " ('gmt', 17684),\n",
       " ('can', 16284),\n",
       " ('will', 14690),\n",
       " ('1993', 14414),\n",
       " ('re:', 14213),\n",
       " ('--', 14076),\n",
       " ('writes:', 14040),\n",
       " ('one', 12976),\n",
       " ('references:', 12548),\n",
       " ('article', 12108),\n",
       " ('|>', 11332),\n",
       " ('sender:', 10907),\n",
       " ('no', 10507),\n",
       " (\"don't\", 9641),\n",
       " ('like', 9434),\n",
       " ('just', 9425),\n",
       " ('nntp-posting-host:', 8598),\n",
       " ('people', 8415),\n",
       " ('get', 8331),\n",
       " ('university', 8203),\n",
       " ('93', 7992),\n",
       " ('know', 7695),\n",
       " ('>>', 7609),\n",
       " ('think', 7205),\n",
       " ('may', 7126),\n",
       " ('use', 6251),\n",
       " ('also', 6166),\n",
       " ('new', 6086),\n",
       " ('xref:', 6054),\n",
       " ('cantaloupe.srv.cs.cmu.edu', 6050),\n",
       " ('even', 5400),\n",
       " ('good', 5278),\n",
       " ('make', 4916),\n",
       " ('many', 4872),\n",
       " ('see', 4691),\n",
       " ('two', 4557),\n",
       " ('much', 4529),\n",
       " ('distribution:', 4406),\n",
       " ('time', 4336),\n",
       " ('it.', 4185),\n",
       " ('first', 4114),\n",
       " ('want', 4104),\n",
       " ('say', 4051),\n",
       " ('anyone', 3976),\n",
       " ('need', 3918),\n",
       " ('way', 3791),\n",
       " ('us', 3745),\n",
       " ('used', 3742),\n",
       " ('go', 3617),\n",
       " ('world', 3602),\n",
       " ('really', 3507),\n",
       " ('now', 3450),\n",
       " ('since', 3418),\n",
       " ('20', 3369),\n",
       " ('16', 3346),\n",
       " ('right', 3326),\n",
       " ('believe', 3308),\n",
       " ('still', 3290),\n",
       " (\"max>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'\", 3289),\n",
       " ('going', 3216),\n",
       " ('something', 3190),\n",
       " ('computer', 3157),\n",
       " ('find', 3145),\n",
       " ('system', 3137),\n",
       " ('21', 3119),\n",
       " ('take', 3115),\n",
       " ('might', 3107),\n",
       " ('said', 3060),\n",
       " ('please', 3055),\n",
       " ('reply-to:', 3050),\n",
       " ('using', 3033),\n",
       " ('god', 2881),\n",
       " ('15', 2881),\n",
       " ('never', 2881),\n",
       " ('last', 2849),\n",
       " ('must', 2846),\n",
       " ('back', 2840),\n",
       " (\"can't\", 2836),\n",
       " ('news', 2836),\n",
       " ('state', 2787),\n",
       " ('without', 2749),\n",
       " ('work', 2692),\n",
       " ('well', 2669),\n",
       " ('got', 2645),\n",
       " (\"doesn't\", 2643),\n",
       " ('someone', 2610),\n",
       " ('>in', 2610),\n",
       " ('off', 2568),\n",
       " ('better', 2545),\n",
       " ('government', 2534)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title\n",
    "sorted_vocab[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = list(kvocab.keys())\n",
    "\n",
    "## Create a Dataframe containing features_list as columns \n",
    "df = pd.DataFrame(columns = features_list)\n",
    "\n",
    "\n",
    "## Filling the x_train values in dataframe \n",
    "\n",
    "for i in range(len(directory)):\n",
    "    ##Create a list of files in the given dictionary \n",
    "    files = os.listdir('C:/Users/nikhi/Downloads/20_newsgroups/' + directory[i])\n",
    " \n",
    "    for j in range(len(files)):\n",
    "        ##Insert a row at the end of Dataframe with all zeros\n",
    "        df.loc[len(df)] = np.zeros(len(features_list))\n",
    "        \n",
    "        ##Path of each file \n",
    "        path = 'C:/Users/nikhi/Downloads/20_newsgroups/' + directory[i] + '/' + files[j]\n",
    "        \n",
    "        ##open the file and read it\n",
    "        text = open(path, 'r', errors='ignore').read()\n",
    "        \n",
    "        \n",
    "        for word in text.split():\n",
    "            if word.lower() in features_list:\n",
    "                df[word.lower()][len(df)-1] += 1\n",
    "                \n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making the 2d array of x\n",
    "x = df.values\n",
    "\n",
    "## Feature list\n",
    "f_list = list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19997,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Creating  y array containing labels for classification \n",
    "\n",
    "y = []\n",
    "\n",
    "for i in range(len(directory)):\n",
    "    ##Create a list of files in the given dictionary \n",
    "    files = os.listdir('C:/Users/nikhi/Downloads/20_newsgroups/' + directory[i])\n",
    " \n",
    "    for j in range(len(files)):\n",
    "        y.append(i)\n",
    "\n",
    "y = np.array(y)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting the whole dataset for training and testing\n",
    "from sklearn import model_selection\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(x, y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Multinomial Naive Bayes from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8641061545642462, 0.817)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "train_score = clf.score(x_train, y_train)\n",
    "test_score = clf.score(x_test, y_test)\n",
    "\n",
    "train_score, test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Multinomial Naive Bayes from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(x_train, y_train):\n",
    "    \n",
    "    ## dictionary containing the count of words\n",
    "    count = {}\n",
    "       \n",
    "    ## set of all classes \n",
    "    set_class = set(y_train)\n",
    "            \n",
    "    for current_class in set_class:\n",
    "        count[current_class] = {}\n",
    "        count[\"total_data\"] = len(y_train)\n",
    "        \n",
    "        ##Rows whose class is current_class\n",
    "        current_class_rows = (y_train == current_class)\n",
    "        \n",
    "        x_train_current = x_train[current_class_rows]\n",
    "        y_train_current = y_train[current_class_rows]\n",
    "        \n",
    "        sums = 0\n",
    "        for i in range(len(f_list)):\n",
    "            ## For each class, calculating total frequency of a feature \n",
    "            count[current_class][f_list[i]] = x_train_current[:,i].sum()\n",
    "            sums = sums + count[current_class][f_list[i]]\n",
    "        \n",
    "        ##Calculating total count of words of a class\n",
    "        count[current_class][\"total_count\"] = sums\n",
    "        \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(dictionary, row, current_class):\n",
    "    ## class_prob = log of probability of the current class = log(no of documents having class as current_class)/ (total number of documents)\n",
    "    class_prob = np.log(dictionary[current_class][\"total_count\"]) - np.log(dictionary[\"total_data\"])\n",
    "    total_prob = class_prob\n",
    "    \n",
    "    \n",
    "    for i in range(len(row)):\n",
    "        ##Numerator\n",
    "        word_count = dictionary[current_class][f_list[i]] + 1     \n",
    "        ## Denominator\n",
    "        total_count = dictionary[current_class][\"total_count\"] + len(f_list)\n",
    "        ## Add 1 to numerator and len(row) in denominator for laplace correction\n",
    "        \n",
    "        ## Log Probabilty of a word \n",
    "        word_prob = np.log(word_count) - np.log(total_count)\n",
    "        \n",
    "        ##Calculating probability frequency number of times\n",
    "        for j in range(int(row[i])):\n",
    "            total_prob += word_prob\n",
    "        \n",
    "    return total_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSinglePoint(row, dictionary):\n",
    "    classes = dictionary.keys()\n",
    "    \n",
    "    ##Initialising best_prob and best_class as very low count\n",
    "    \n",
    "    best_prob = -1000\n",
    "    best_class = -1\n",
    "    first_iter = True\n",
    "    \n",
    "    for current_class in classes:\n",
    "        if(current_class == \"total_data\"):\n",
    "            continue\n",
    "        \n",
    "        ##Calculating probabilty that the given row belong to current_class\n",
    "        prob_current_class = probability(dictionary, row, current_class)\n",
    "        \n",
    "        ##For first iteration we set the best_prob to be the probabilty that row is of first class and best_class to be first class\n",
    "        ##For rest iteration, we check if the probabilty that row is of the current_class is greater than the best_prob then we update best_prob and best_class.\n",
    "        if(first_iter or prob_current_class > best_prob):\n",
    "            best_prob = prob_current_class\n",
    "            best_class = current_class\n",
    "        \n",
    "        first_iter = False\n",
    "    \n",
    "    ## Return the best class which has maximum probabilty.\n",
    "    return best_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x_test, dictionary):\n",
    "    ## Initialise a list which contain the predictions\n",
    "    y_pred_self = []\n",
    "    \n",
    "    ##Iterate through each row in x_test\n",
    "    for j in range(len(x_test)):\n",
    "        \n",
    "        ##Calculate the prediction of the class to which the row belong to.\n",
    "        pred_class = predictSinglePoint(x_test[j,:], dictionary) \n",
    "        \n",
    "        ##Append the predicted class to our list\n",
    "        y_pred_self.append(pred_class)\n",
    "    \n",
    "    ##Return the list of predictions\n",
    "    return y_pred_self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training the model \n",
    "dictionary = fit(x_train, y_train)\n",
    "\n",
    "##Testing the model \n",
    "y_pred_self = predict(x_test, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy Comparison between Sklearn MultinomialNB() and self implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for sklearn MultinomialNB() -  0.817\n",
      "Accuracy for self-implemented Naive Bayes -  0.8206\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy for sklearn MultinomialNB() - \", test_score)\n",
    "print(\"Accuracy for self-implemented Naive Bayes - \", accuracy_score(y_test, y_pred_self))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Report Comparison between Sklearn MultinomialNB() and self implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for sklearn MultinomialNB() \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.76      0.73       233\n",
      "           1       0.78      0.74      0.76       253\n",
      "           2       0.87      0.78      0.82       249\n",
      "           3       0.81      0.85      0.83       240\n",
      "           4       0.81      0.89      0.85       236\n",
      "           5       0.90      0.79      0.84       240\n",
      "           6       0.66      0.88      0.76       261\n",
      "           7       0.81      0.88      0.84       269\n",
      "           8       0.80      0.93      0.86       284\n",
      "           9       0.94      0.94      0.94       248\n",
      "          10       0.95      0.94      0.94       231\n",
      "          11       0.95      0.88      0.91       233\n",
      "          12       0.80      0.86      0.83       244\n",
      "          13       0.87      0.83      0.85       256\n",
      "          14       0.86      0.85      0.86       246\n",
      "          15       0.95      0.98      0.97       252\n",
      "          16       0.72      0.82      0.76       249\n",
      "          17       0.93      0.75      0.83       281\n",
      "          18       0.69      0.55      0.62       259\n",
      "          19       0.58      0.44      0.50       236\n",
      "\n",
      "    accuracy                           0.82      5000\n",
      "   macro avg       0.82      0.82      0.81      5000\n",
      "weighted avg       0.82      0.82      0.81      5000\n",
      "\n",
      "Classification report for self-implemented Naive Bayes \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.76      0.73       233\n",
      "           1       0.78      0.75      0.76       253\n",
      "           2       0.86      0.78      0.82       249\n",
      "           3       0.82      0.86      0.84       240\n",
      "           4       0.83      0.89      0.86       236\n",
      "           5       0.91      0.81      0.85       240\n",
      "           6       0.68      0.87      0.77       261\n",
      "           7       0.80      0.88      0.84       269\n",
      "           8       0.82      0.93      0.87       284\n",
      "           9       0.94      0.94      0.94       248\n",
      "          10       0.95      0.94      0.94       231\n",
      "          11       0.95      0.88      0.91       233\n",
      "          12       0.81      0.86      0.83       244\n",
      "          13       0.87      0.83      0.85       256\n",
      "          14       0.86      0.86      0.86       246\n",
      "          15       0.95      0.98      0.97       252\n",
      "          16       0.71      0.82      0.76       249\n",
      "          17       0.93      0.75      0.83       281\n",
      "          18       0.69      0.56      0.62       259\n",
      "          19       0.58      0.44      0.50       236\n",
      "\n",
      "    accuracy                           0.82      5000\n",
      "   macro avg       0.82      0.82      0.82      5000\n",
      "weighted avg       0.82      0.82      0.82      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\"Classification report for sklearn MultinomialNB() \")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Classification report for self-implemented Naive Bayes \")\n",
    "print(classification_report(y_test, y_pred_self))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
